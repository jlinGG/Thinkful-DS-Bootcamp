{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sayari programming assignment\n",
    "### Joanne Lin\n",
    "\n",
    "This project utilizes scrapy and Xpath to crawl New Zealand's public registry of incorporated companies.  The only module needed is scrapy. You can use the accompanying requirements.txt and run \"pip install -r /path/to/requirements.txt\" in the command line to insure the proper environment for these spiders. \n",
    "\n",
    "Note: If running the spiders on Jupyter Notebook, you would need to restart your kernel each time you run the spider.  This is not an issue when running the spiders in the Command Line. \n",
    "\n",
    "## html spider\n",
    "The html spider crawls the company website and retrieves the raw html, storing it in a company.html file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class htmlSpider(scrapy.Spider):\n",
    "    name = \"html\"\n",
    "    \n",
    "    # The 'detail?' at the end of the URL pulls up all the company information on a single page.\n",
    "    start_urls = [\n",
    "        'https://app.companiesoffice.govt.nz/companies/app/ui/pages/companies/6236487/detail?',\n",
    "    ]\n",
    "\n",
    "    # Download all the code and save it to the company.html file\n",
    "    def parse(self, response):\n",
    "        with open('company.html', 'wb') as f:\n",
    "            f.write(response.body)\n",
    "\n",
    "\n",
    "# Instantiate our crawler.\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(htmlSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NZSpider\n",
    "\n",
    "This spider crawls the company page for information and exports into NZcompanies.json ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class NZSpider(scrapy.Spider):\n",
    "    name = \"NZS\"\n",
    "    \n",
    "    # The 'detail?' at the end of the URL pulls up all the company information on a single page.\n",
    "    start_urls = [\n",
    "        'https://app.companiesoffice.govt.nz/companies/app/ui/pages/companies/6236487/detail?'\n",
    "        ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # This creates an .html file for the original HTML\n",
    "        with open('companypage.html', 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        \n",
    "        # Looping through all the panels, or pageContainers on page\n",
    "        for panel in response.xpath('//*[@id=\"maincol\"]/div[@class=\"entity\"/div[@class=\"pageCountainer\"'):\n",
    "\n",
    "            # Scraping data from main panel\n",
    "            if response.xpath('div[@id=\"onLoadSetFocus]'):\n",
    "\n",
    "                # Yield a dictionary with the values\n",
    "                yield {\n",
    "                    'CompanyNumber' : panel.xpath('//*[@class=\"readonly companySummary]/div[1]/text()').extract(),\n",
    "                    'NZBusinessNumber' : panel.xpath('//*[@class=\"readonly companySummary]/div[2]/text()').extract(),\n",
    "                    'IncorporationDate' : panel.xpath('//*[@class=\"readonly companySummary]/div[3]/text()').extract(),\n",
    "                    'CompanyStatus' : panel.xpath('//*[@class=\"readonly companySummary]/div[4]/text()').extract(),\n",
    "                    'EntityType' : panel.xpath('//*[@class=\"readonly companySummary]/div[5]/text()').extract(),\n",
    "                    'ConstitutionFiled' : panel.xpath('//*[@class=\"readonly companySummary]/div[6]/text()').extract(),\n",
    "                    'ARFilingMonth' : panel.xpath('//*[@class=\"readonly companySummary]/div[7]/table/tbody/tr/td[1]/text()').extract(),\n",
    "                    'UltimateHoldingCompany' : panel.xpath('//*[@class=\"readonly companySummary]/div[8]/div/text()').extract()\n",
    "                }\n",
    "\n",
    "            elif response.xpath('div[@id=\"directorsPanel]'):\n",
    "                # Loop over each director panel in director tab\n",
    "                for director in response.xpath('//*[@id=\"director\"]'):\n",
    "                    yield {\n",
    "                        'DirectorName' : panel.xpath('table/tbody/tr/td[1]/div[1]/text()').extract(),\n",
    "                        'Address' : panel.xpath('table/tbody/tr/td[1]/div[2]/text()').extract(),\n",
    "                        'ApptDate' : panel.xpath('table/tbody/tr/td[1]/div[3]/text()').extract(),\n",
    "                        'Address' : panel.xpath('x_pathlink').extract()\n",
    "                    }\n",
    "\n",
    "            elif response.xpath('div[@id=\"shareholdingTab\"]'):\n",
    "                # Loop over each shareholder panel in shareholding tab\n",
    "                for shareholder in response.xpath('//*[@id=\"allocations\"]'):\n",
    "                    yield {\n",
    "                        'ShareNumber' : shareholder.xpath('//*[@id=\"allocations\"]/div[1]/div[1]/text()').extract(),\n",
    "                        'SharePortion' : shareholder.xpath('//*[@id=\"allocations\"]/div[1]/div[1]/span/text()[2]').extract(),\n",
    "                        'Name' : shareholder.xpath('//*[@id=\"allocations\"]/div[1]/div[3]/div').extract(),\n",
    "                        'Address' : shareholder.xpath('//*[@id=\"allocations\"]/div[1]/div[4]/div').extract(),\n",
    "                    }\n",
    "\n",
    "            elif response.xpath('div[@id=\"addressPanel\"]'):\n",
    "                # Loop over addresses\n",
    "                for address in response.xpath('div[@class=\"row\"]'):\n",
    "                    yield {\n",
    "                        'Address' : address.xpath('//*[@id=\"addressPanel\"]/div[2]/div').extract()\n",
    "                    }\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "                                              \n",
    "# Tell the script how to run the crawler by passing in settings. \n",
    "# Some of these settings, as indicated, insure that we are not \n",
    "#    bombarding the website with increases in traffic.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'NZCompanies.json',       # Name of storage file.\n",
    "    'LOG_ENABLED': False,          \n",
    "    'ROBOTSTXT_OBEY': True, # Scrapy is set to automatically obey robot.txt files.\n",
    "    'USER_AGENT': 'SayariAnalyticsCrawler (www.sayarianalytics.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True, # Autothrottle dynamically sets the delay based on how quickly the server responds\n",
    "    'HTTPCACHE_ENABLED': True #Prevents server from downloading the same exact pages.\n",
    "})\n",
    "\n",
    "# Start the spider.\n",
    "process.crawl(NZSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kiwi companies SQL database schema\n",
    "Below is my rendering of what the SQL databse schema would look like.  There would be four tables (Company, Directors, Shareholder, and Allocation) connected with unique ids.\n",
    "\n",
    "Company\n",
    "- co_id  #Unique, autogenerated company id\n",
    "- company_no\n",
    "- nzbn\n",
    "- incorp_date\n",
    "- entity_type \n",
    "- constitution_filed \n",
    "- ar_filing_mo\n",
    "- ult_holding\n",
    "- street\n",
    "- city\n",
    "- state\n",
    "- country\n",
    "- zip\n",
    "- total_shares\n",
    "\n",
    "Director\n",
    "- director_id  #Unique, autogenerated director id\n",
    "- first_name\n",
    "- last_name\n",
    "- middle_name\n",
    "- appt_date\n",
    "- shareholder_id   #Linked to shareholder table\n",
    "- company_id   #Linked to company table\n",
    "- street\n",
    "- city\n",
    "- state\n",
    "- country\n",
    "- zip\n",
    "\n",
    "Shareholder\n",
    "- sh_id  #Unique, autogenerated shareholder id\n",
    "- company_id   #Linked to company table\n",
    "- first_name\n",
    "- last_name\n",
    "- middle_name\n",
    "- street\n",
    "- city\n",
    "- state\n",
    "- country\n",
    "- zip\n",
    "\n",
    "Allocation\n",
    "- alloc_id   #Unique, autogenerated allocation id\n",
    "- shareholder_id   #Linked to shareholder table\n",
    "- company_id   #Linked to company table\n",
    "- shares_owned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "While the NZSpider does run successfully, the generated json file is empty.  In the time given, I can't figure out why that is. I suspect there is something wrong with the XPath syntax.  The htmlSpider does generate the html file.\n",
    "\n",
    "I also couldn't figure out how to run the spider recursively on this webpage, as the website is not paginated.  After scraping the information of this company, how do we move on to the next one? Ideally, the spider would crawl the entire website and capture all the company data once.  If we want to update the database, the spider will crawl the website and only update companies where it detects there are changes, and add new companies that aren't in the current database. \n",
    "\n",
    "Aside from utilizing the Twitter API, this is the first spider I built from scratch. I learned a lot! I look forward to learning more about how it can be improved.  Thanks for the opportunity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
